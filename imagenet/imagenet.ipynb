{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\"\"\"\n",
    "- Input image : 224 x 224 x 3\n",
    "- Kernels : 11x 11 x 3\n",
    "- Stride = 4\n",
    "- out_channel = 96\n",
    "\n",
    "\n",
    "- Input image : 224 x 224 x 3\n",
    "- Kernels : 5x 5x 3\n",
    "- Stride = 1\n",
    "- out_channel = 256\n",
    "\"\"\"\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self , classes) -> None:\n",
    "        super().__init__()\n",
    "        self.convs = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11 , stride=4), # b * 96 * 55 * 55\n",
    "        nn.LocalResponseNorm(size=5, k=2),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2), # b* 96 * 27 * 27\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5),\n",
    "        nn.LocalResponseNorm(size=5, k=2),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),  # N x 1024\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=(256 * 2* 2), out_features=4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(in_features=4096, out_features=4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=4096, out_features=classes),  # Adjusted for the number of classes\n",
    "        )\n",
    "        self.init_parameter()\n",
    "\n",
    "    def init_parameter(self):\n",
    "        for layer in self.convs:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.normal_(layer.weight, mean=0, std=0.1),\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "            \n",
    "        nn.init.constant_(self.convs[4].bias, 1)\n",
    "        nn.init.constant_(self.convs[10].bias, 1)\n",
    "        nn.init.constant_(self.convs[12].bias, 1)\n",
    "\n",
    "    def forward(self, x:Tensor):\n",
    "        x = self.convs(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m     total_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[0;32m--> 112\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_histogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m         writer\u001b[38;5;241m.\u001b[39madd_images(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m, X, global_step\u001b[38;5;241m=\u001b[39mtotal_steps)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Validate the model\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/machine-learning-papers/.venv/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py:526\u001b[0m, in \u001b[0;36mSummaryWriter.add_histogram\u001b[0;34m(self, tag, values, global_step, bins, walltime, max_bins)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bins, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m bins \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    524\u001b[0m     bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_bins\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(\n\u001b[0;32m--> 526\u001b[0m     \u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_bins\u001b[49m\u001b[43m)\u001b[49m, global_step, walltime\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/machine-learning-papers/.venv/lib/python3.9/site-packages/torch/utils/tensorboard/summary.py:483\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(name, values, bins, max_bins)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Output a `Summary` protocol buffer with a histogram.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03mThe generated\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m  buffer.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m values \u001b[38;5;241m=\u001b[39m make_np(values)\n\u001b[0;32m--> 483\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmake_histogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Summary(value\u001b[38;5;241m=\u001b[39m[Summary\u001b[38;5;241m.\u001b[39mValue(tag\u001b[38;5;241m=\u001b[39mname, histo\u001b[38;5;241m=\u001b[39mhist)])\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/machine-learning-papers/.venv/lib/python3.9/site-packages/torch/utils/tensorboard/summary.py:492\u001b[0m, in \u001b[0;36mmake_histogram\u001b[0;34m(values, bins, max_bins)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input has no element.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    491\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 492\u001b[0m counts, limits \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m num_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(counts)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_bins \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m num_bins \u001b[38;5;241m>\u001b[39m max_bins:\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/machine-learning-papers/.venv/lib/python3.9/site-packages/numpy/lib/histograms.py:867\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, density, weights)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _range(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(a), BLOCK):\n\u001b[0;32m--> 867\u001b[0m         sa \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mBLOCK\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m         cum_n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _search_sorted_inclusive(sa, bin_edges)\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/Machine Learning/machine-learning-papers/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1016\u001b[0m, in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1016\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mK\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m a\u001b[38;5;241m.\u001b[39msort(axis\u001b[38;5;241m=\u001b[39maxis, kind\u001b[38;5;241m=\u001b[39mkind, order\u001b[38;5;241m=\u001b[39morder)\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "\n",
    "GPUS = [0]\n",
    "EPOCHS = 90\n",
    "NO_CLASSES = 1000\n",
    "TRAIN_DIR = 'imagenet-mini/train'\n",
    "VAL_DIR = 'imagenet-mini/val'\n",
    "IMG_DIM = 227\n",
    "BATCH_SIZE = 128\n",
    "L_RATE = 0.005\n",
    "W_DECAY = 0.0005\n",
    "MOMENTUM = 0.9\n",
    "CHECKPOINT_DIR = 'checkpoints/'\n",
    "\n",
    "data_dir = \"imagenet1k\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "seed = torch.initial_seed()\n",
    "\n",
    "\n",
    "# create model\n",
    "model = AlexNet(classes=NO_CLASSES)\n",
    "\n",
    "\n",
    "# train with multi GPU\n",
    "model = torch.nn.parallel.DataParallel(model, device_ids=GPUS)\n",
    "\n",
    "# image augmentation and tranformation\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.CenterCrop(IMG_DIM),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# parepare the dataset\n",
    "imagenet_data = datasets.ImageFolder(root=data_dir, transform=data_transform)\n",
    "\n",
    "train_size = int(0.7 * len(imagenet_data))\n",
    "val_size = int(0.15 * len(imagenet_data))\n",
    "test_size = len(imagenet_data) - train_size - val_size\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_set, val_set, test_set = random_split(imagenet_data, [train_size, val_size, test_size],generator=torch.Generator(device=device),)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # num_workers=8\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # num_workers=8\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    # num_workers=8\n",
    ")\n",
    "\n",
    "# optimizer\n",
    "# optim = torch.optim.SGD(\n",
    "#     model.parameters(),\n",
    "#     lr=L_RATE,\n",
    "#     momentum=MOMENTUM,\n",
    "#     weight_decay=W_DECAY\n",
    "# )\n",
    "\n",
    "optim = torch.optim.Adam(params=model.parameters(), lr=0.0001)\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# decay the learning rate\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=30, gamma=0.1)\n",
    "\n",
    "total_steps =1\n",
    "writer = SummaryWriter(\"tensorboard\")\n",
    "\n",
    "# training\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    for step, (X, y) in enumerate(train_loader):\n",
    "        X, y = X, y\n",
    "        # refreshing gradients\n",
    "        optim.zero_grad()\n",
    "        # forward_pass\n",
    "        pred = model(X)\n",
    "        # taking loss\n",
    "        loss:Tensor = criterion(pred, y)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # taking step\n",
    "        optim.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        if total_steps % 10 == 0:\n",
    "            print(f'step: {total_steps} | Loss: {loss}')\n",
    "            # writer.add_scalar(\"Loss/train\", loss, step)\n",
    "        total_steps += 1\n",
    "\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     writer.add_histogram(name, param, epoch)\n",
    "        #     writer.add_images('images', X, global_step=total_steps)\n",
    "\n",
    "     \n",
    "\n",
    "    # Validate the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            outputs = model(images)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, \"\n",
    "          f\"Training Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "          f\"Validation Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "          f\"Validation Accuracy: {(correct/total) * 100:.2f}%\")   \n",
    "    \n",
    "    # saving checkpoints\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f'model_checkpoint{epoch+1}.pkl')\n",
    "    state = {\n",
    "        'epoch': epoch,\n",
    "        'total_steps': total_steps,\n",
    "        'optimizer': optim.load_state_dict,\n",
    "        'model': model.state_dict(),\n",
    "        'seed': seed\n",
    "    }\n",
    "    writer.close()\n",
    "    torch.save(state, checkpoint_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
